{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bbccc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6056095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('fashion-mnist_train.csv')\n",
    "testing = pd.read_csv('fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350f5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = training.loc[:, training.columns != 'label'], training.loc[:, training.columns == 'label']\n",
    "X_test, y_test = testing.loc[:, testing.columns != 'label'], testing.loc[:, testing.columns == 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed3e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86083d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train/255, X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3959323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "y_train = onehot_encoder.fit_transform(y_train).toarray()\n",
    "y_test = onehot_encoder.fit_transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.toarray()\n",
    "# y_test = y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f880bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381049ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000 | loss: 0.2650490403175354 |Accuracy: 90.0%\n",
      "Iteration: 2000 | loss: 0.27972570061683655 |Accuracy: 90.0%\n",
      "Iteration: 3000 | loss: 0.3511337637901306 |Accuracy: 86.0%\n",
      "Iteration: 4000 | loss: 0.40125328302383423 |Accuracy: 82.0%\n",
      "Iteration: 5000 | loss: 0.5328548550605774 |Accuracy: 86.0%\n",
      "Epoch 1 completed.\n",
      "Iteration: 1000 | loss: 0.23673681914806366 |Accuracy: 94.0%\n",
      "Iteration: 2000 | loss: 0.2324506938457489 |Accuracy: 90.0%\n",
      "Iteration: 3000 | loss: 0.2109733521938324 |Accuracy: 90.0%\n",
      "Iteration: 4000 | loss: 0.30207526683807373 |Accuracy: 92.0%\n",
      "Iteration: 5000 | loss: 0.22341850399971008 |Accuracy: 92.0%\n",
      "Epoch 2 completed.\n",
      "Iteration: 1000 | loss: 0.34380996227264404 |Accuracy: 84.0%\n",
      "Iteration: 2000 | loss: 0.6069839000701904 |Accuracy: 78.0%\n",
      "Iteration: 3000 | loss: 0.19872169196605682 |Accuracy: 94.0%\n",
      "Iteration: 4000 | loss: 0.5647631883621216 |Accuracy: 84.0%\n",
      "Iteration: 5000 | loss: 0.34545010328292847 |Accuracy: 88.0%\n",
      "Epoch 3 completed.\n",
      "Iteration: 1000 | loss: 0.3122999668121338 |Accuracy: 92.0%\n",
      "Iteration: 2000 | loss: 0.20961183309555054 |Accuracy: 92.0%\n",
      "Iteration: 3000 | loss: 0.3894461393356323 |Accuracy: 88.0%\n",
      "Iteration: 4000 | loss: 0.300631582736969 |Accuracy: 82.0%\n",
      "Iteration: 5000 | loss: 0.12234779447317123 |Accuracy: 98.0%\n",
      "Epoch 4 completed.\n",
      "Iteration: 1000 | loss: 0.336651086807251 |Accuracy: 88.0%\n",
      "Iteration: 2000 | loss: 0.4151568114757538 |Accuracy: 84.0%\n",
      "Iteration: 3000 | loss: 0.5017175078392029 |Accuracy: 82.0%\n",
      "Iteration: 4000 | loss: 0.36867621541023254 |Accuracy: 88.0%\n",
      "Iteration: 5000 | loss: 0.3278948962688446 |Accuracy: 80.0%\n",
      "Epoch 5 completed.\n"
     ]
    }
   ],
   "source": [
    "model = FCNN(X_train.shape[1])\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 5\n",
    "n_iters = 5001\n",
    "batch_size = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(n_iters):\n",
    "        idx = torch.randperm(X_train.shape[0])[:batch_size].numpy()\n",
    "        X_batch = torch.tensor(X_train[idx][:batch_size]).type(torch.float32)\n",
    "        y_batch = torch.tensor(y_train[idx][:batch_size]).type(torch.float32)\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            _, predictions = torch.max(y_pred, 1)\n",
    "            _, labels = torch.max(y_batch, 1)\n",
    "            accuracy = predictions.eq(labels).sum() / batch_size * 100\n",
    "            print(f'Iteration: {i} | loss: {loss} |Accuracy: {accuracy}%')\n",
    "            \n",
    "    print(f'Epoch {epoch+1} completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b51772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.93000030517578%\n"
     ]
    }
   ],
   "source": [
    "_, test_pred = torch.max(model(torch.tensor(X_test).type(torch.float32)),1)\n",
    "_, test_labels = torch.max(torch.tensor(y_test), 1)\n",
    "acc = test_pred.eq(test_labels).sum() / y_test.shape[0] * 100\n",
    "print(f'Test Accuracy: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e29665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model to FHE.\n"
     ]
    }
   ],
   "source": [
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "\n",
    "print('Compiling the model to FHE.')\n",
    "\n",
    "try:\n",
    "    quantised_compiled_module = compile_torch_model(\n",
    "        model,\n",
    "        X_train,\n",
    "        n_bits=3\n",
    "    )\n",
    "    print('The network is trained and FHE friendly.')\n",
    "except RuntimeError as e:\n",
    "    if str(e).startswith(\"max_bit_width of some nodes is too high\"):\n",
    "        print(\"The network is not fully FHE friendly, retraining.\")\n",
    "    raise e\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Could not compile the model to FHE.\"\n",
    "        \"You may need to decrease the n_bits parameter to avoid potential overflows.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa597351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concrete.ml.torch.compile import compile_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352387ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concrete.common.compilation import CompilationConfiguration\n",
    "\n",
    "cfg = CompilationConfiguration(\n",
    "    dump_artifacts_on_unexpected_failures=False,\n",
    "    enable_unsafe_features=True,  # This is for our tests only, never use that in prod\n",
    "    treat_warnings_as_errors=True,\n",
    "    use_insecure_key_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "\n",
    "def test_with_concrete_virtual_lib(quantised_module, use_fhe, use_vl):\n",
    "    dtype_inputs = np.uint8 if use_fhe else np.int32\n",
    "    all_y_pred = np.zeros((len(X_test)), dtype=np.int32)\n",
    "    all_targets = np.zeros((len(X_test)), dtype=np.int32)\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(len(X_test)):\n",
    "        sample_q = quantised_module.quantize_input(X_test[i]).astype(dtype_inputs)\n",
    "        \n",
    "        endidx = idx + y_test.shape[1]\n",
    "        all_targets[idx:endidx] = y_test[i]\n",
    "        \n",
    "        for j in range(len(X_test)):\n",
    "            x_q = np.expand_dims(sample_q[j,:], 0)\n",
    "            if use_fhe or use_vl:\n",
    "                out_fhe = quantised_module.forward_fhe.encrypt_run_decrypt(x_q)\n",
    "                output = quantised_module.dequantize_output(out_fhe)\n",
    "            else:\n",
    "                output = quantised_module.forward_and_dequant(x_q)\n",
    "            \n",
    "            y_pred = np.argmax(output, 1)\n",
    "            all_y_pred[idx] = y_pred\n",
    "            idx += 1\n",
    "    \n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "    return n_correct / len(X_test)\n",
    "\n",
    "\n",
    "accs = []\n",
    "accum_bits = []\n",
    "for n_bits in range(2,3):\n",
    "    q_module_vl = compile_torch_model(\n",
    "        model,\n",
    "        X_train,\n",
    "        n_bits=n_bits,\n",
    "        use_virtual_lib = True,\n",
    "        compilation_configuration=cfg\n",
    "    )\n",
    "    accum_bits.append(q_module_vl.forward_fhe.get_max_bit_width())\n",
    "    accs.append(\n",
    "        test_with_concrete_virtual_lib(\n",
    "            q_module_vl,\n",
    "            use_fhe=False,\n",
    "            use_vl=True,\n",
    "        )\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.plot(range(2, 9), accs, \"-x\")\n",
    "for bits, acc, accum in zip(range(2, 9), accs, accum_bits):\n",
    "    plt.gca().annotate(str(accum), (bits - 0.1, acc + 0.025))\n",
    "plt.ylabel(\"Accuracy on test set\")\n",
    "plt.xlabel(\"Weight & activation quantization\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Accuracy for varying quantization bit width\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d33a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.expand_dims(X_test_q[j,], 0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41e47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
